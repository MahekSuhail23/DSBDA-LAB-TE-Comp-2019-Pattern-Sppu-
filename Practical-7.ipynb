{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5dfee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programmer:Mahek Suhail Shaikh\n",
    "#College: Keystone School Of Engineering ,Pune\n",
    "#Class:TE-B\n",
    "#RollNo: 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f4d73",
   "metadata": {},
   "source": [
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7025f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk ->natural language  toolkit\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff611e8",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb0c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31e1da20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As I have mentioned earlier Science has got many changes in our lives. First of all, transportation is easier now. With the help of Science it now easier to travel long distances. Moreover, the time of traveling is also reduced. Various high-speed vehicles are available these days. These vehicles have totally changed. The phase of our society. Science upgraded steam engines to electric engines. In earlier times people were traveling with cycles. But now everybody travels on motorcycles and cars. This saves time and effort. And this is all possible with the help of Science.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus='As I have mentioned earlier Science has got many changes in our lives. First of all, transportation is easier now. With the help of Science it now easier to travel long distances. Moreover, the time of traveling is also reduced. Various high-speed vehicles are available these days. These vehicles have totally changed. The phase of our society. Science upgraded steam engines to electric engines. In earlier times people were traveling with cycles. But now everybody travels on motorcycles and cars. This saves time and effort. And this is all possible with the help of Science.'\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ebc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization : \n",
      " ['As', 'I', 'have', 'mentioned', 'earlier', 'Science', 'has', 'got', 'many', 'changes', 'in', 'our', 'lives', '.', 'First', 'of', 'all', ',', 'transportation', 'is', 'easier', 'now', '.', 'With', 'the', 'help', 'of', 'Science', 'it', 'now', 'easier', 'to', 'travel', 'long', 'distances', '.', 'Moreover', ',', 'the', 'time', 'of', 'traveling', 'is', 'also', 'reduced', '.', 'Various', 'high-speed', 'vehicles', 'are', 'available', 'these', 'days', '.', 'These', 'vehicles', 'have', 'totally', 'changed', '.', 'The', 'phase', 'of', 'our', 'society', '.', 'Science', 'upgraded', 'steam', 'engines', 'to', 'electric', 'engines', '.', 'In', 'earlier', 'times', 'people', 'were', 'traveling', 'with', 'cycles', '.', 'But', 'now', 'everybody', 'travels', 'on', 'motorcycles', 'and', 'cars', '.', 'This', 'saves', 'time', 'and', 'effort', '.', 'And', 'this', 'is', 'all', 'possible', 'with', 'the', 'help', 'of', 'Science', '.'] \n",
      "\n",
      "Sentence Tokenization : \n",
      " ['As I have mentioned earlier Science has got many changes in our lives.', 'First of all, transportation is easier now.', 'With the help of Science it now easier to travel long distances.', 'Moreover, the time of traveling is also reduced.', 'Various high-speed vehicles are available these days.', 'These vehicles have totally changed.', 'The phase of our society.', 'Science upgraded steam engines to electric engines.', 'In earlier times people were traveling with cycles.', 'But now everybody travels on motorcycles and cars.', 'This saves time and effort.', 'And this is all possible with the help of Science.']\n"
     ]
    }
   ],
   "source": [
    "print('Word Tokenization : \\n',word_tokenize(corpus),'\\n')\n",
    "print('Sentence Tokenization : \\n',sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35eef17",
   "metadata": {},
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac651d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('As', 'IN'), ('I', 'PRP'), ('have', 'VBP'), ('mentioned', 'VBN'), ('earlier', 'RBR'), ('Science', 'NNP'), ('has', 'VBZ'), ('got', 'VBD'), ('many', 'JJ'), ('changes', 'NNS'), ('in', 'IN'), ('our', 'PRP$'), ('lives', 'NNS'), ('.', '.'), ('First', 'NNP'), ('of', 'IN'), ('all', 'DT'), (',', ','), ('transportation', 'NN'), ('is', 'VBZ'), ('easier', 'JJR'), ('now', 'RB'), ('.', '.'), ('With', 'IN'), ('the', 'DT'), ('help', 'NN'), ('of', 'IN'), ('Science', 'NNP'), ('it', 'PRP'), ('now', 'RB'), ('easier', 'VBZ'), ('to', 'TO'), ('travel', 'VB'), ('long', 'JJ'), ('distances', 'NNS'), ('.', '.'), ('Moreover', 'RB'), (',', ','), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('traveling', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('reduced', 'VBN'), ('.', '.'), ('Various', 'JJ'), ('high-speed', 'JJ'), ('vehicles', 'NNS'), ('are', 'VBP'), ('available', 'JJ'), ('these', 'DT'), ('days', 'NNS'), ('.', '.'), ('These', 'DT'), ('vehicles', 'NNS'), ('have', 'VBP'), ('totally', 'RB'), ('changed', 'VBN'), ('.', '.'), ('The', 'DT'), ('phase', 'NN'), ('of', 'IN'), ('our', 'PRP$'), ('society', 'NN'), ('.', '.'), ('Science', 'NN'), ('upgraded', 'VBD'), ('steam', 'NN'), ('engines', 'NNS'), ('to', 'TO'), ('electric', 'JJ'), ('engines', 'NNS'), ('.', '.'), ('In', 'IN'), ('earlier', 'JJR'), ('times', 'NNS'), ('people', 'NNS'), ('were', 'VBD'), ('traveling', 'VBG'), ('with', 'IN'), ('cycles', 'NNS'), ('.', '.'), ('But', 'CC'), ('now', 'RB'), ('everybody', 'NN'), ('travels', 'NNS'), ('on', 'IN'), ('motorcycles', 'NNS'), ('and', 'CC'), ('cars', 'NNS'), ('.', '.'), ('This', 'DT'), ('saves', 'VBZ'), ('time', 'NN'), ('and', 'CC'), ('effort', 'NN'), ('.', '.'), ('And', 'CC'), ('this', 'DT'), ('is', 'VBZ'), ('all', 'DT'), ('possible', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('help', 'NN'), ('of', 'IN'), ('Science', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = word_tokenize(corpus)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78ed96",
   "metadata": {},
   "source": [
    "stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce4a49fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'I', 'mentioned', 'earlier', 'Science', 'got', 'many', 'changes', 'lives', '.', 'First', ',', 'transportation', 'easier', '.', 'With', 'help', 'Science', 'easier', 'travel', 'long', 'distances', '.', 'Moreover', ',', 'time', 'traveling', 'also', 'reduced', '.', 'Various', 'high-speed', 'vehicles', 'available', 'days', '.', 'These', 'vehicles', 'totally', 'changed', '.', 'The', 'phase', 'society', '.', 'Science', 'upgraded', 'steam', 'engines', 'electric', 'engines', '.', 'In', 'earlier', 'times', 'people', 'traveling', 'cycles', '.', 'But', 'everybody', 'travels', 'motorcycles', 'cars', '.', 'This', 'saves', 'time', 'effort', '.', 'And', 'possible', 'help', 'Science', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "     \n",
    "\n",
    "t = word_tokenize(corpus)\n",
    "colltoken = []\n",
    "for i in t:\n",
    "  if (i not in stop_words):\n",
    "    colltoken.append(i)\n",
    "print(colltoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10751d57",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cc5b4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'i', 'mention', 'earlier', 'scienc', 'got', 'mani', 'chang', 'live', '.', 'first', ',', 'transport', 'easier', '.', 'with', 'help', 'scienc', 'easier', 'travel', 'long', 'distanc', '.', 'moreov', ',', 'time', 'travel', 'also', 'reduc', '.', 'variou', 'high-spe', 'vehicl', 'avail', 'day', '.', 'these', 'vehicl', 'total', 'chang', '.', 'the', 'phase', 'societi', '.', 'scienc', 'upgrad', 'steam', 'engin', 'electr', 'engin', '.', 'in', 'earlier', 'time', 'peopl', 'travel', 'cycl', '.', 'but', 'everybodi', 'travel', 'motorcycl', 'car', '.', 'thi', 'save', 'time', 'effort', '.', 'and', 'possibl', 'help', 'scienc', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "     \n",
    "stemmer = PorterStemmer()\n",
    "     \n",
    "\n",
    "stemmed_tokens = []\n",
    "for i in colltoken:\n",
    "  stemmed = stemmer.stem(i)\n",
    "  stemmed_tokens.append(stemmed)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf3277",
   "metadata": {},
   "source": [
    "Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28eb7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'I', 'mentioned', 'earlier', 'Science', 'got', 'many', 'change', 'life', '.', 'First', ',', 'transportation', 'easier', '.', 'With', 'help', 'Science', 'easier', 'travel', 'long', 'distance', '.', 'Moreover', ',', 'time', 'traveling', 'also', 'reduced', '.', 'Various', 'high-speed', 'vehicle', 'available', 'day', '.', 'These', 'vehicle', 'totally', 'changed', '.', 'The', 'phase', 'society', '.', 'Science', 'upgraded', 'steam', 'engine', 'electric', 'engine', '.', 'In', 'earlier', 'time', 'people', 'traveling', 'cycle', '.', 'But', 'everybody', 'travel', 'motorcycle', 'car', '.', 'This', 'save', 'time', 'effort', '.', 'And', 'possible', 'help', 'Science', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "     \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "     \n",
    "\n",
    "lemmatized_tokens = []\n",
    "for i in colltoken:\n",
    "  lemmatized = lemmatizer.lemmatize(i)\n",
    "  lemmatized_tokens.append(lemmatized)\n",
    "print(lemmatized_tokens)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cf9ee",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f064ba2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'secondly': 19,\n",
       " 'science': 18,\n",
       " 'made': 9,\n",
       " 'us': 25,\n",
       " 'reach': 17,\n",
       " 'to': 24,\n",
       " 'the': 21,\n",
       " 'moon': 11,\n",
       " 'but': 3,\n",
       " 'we': 27,\n",
       " 'never': 12,\n",
       " 'stopped': 20,\n",
       " 'there': 22,\n",
       " 'it': 8,\n",
       " 'also': 1,\n",
       " 'gave': 4,\n",
       " 'glance': 5,\n",
       " 'at': 2,\n",
       " 'mars': 10,\n",
       " 'this': 23,\n",
       " 'is': 7,\n",
       " 'one': 14,\n",
       " 'of': 13,\n",
       " 'greatest': 6,\n",
       " 'achievements': 0,\n",
       " 'was': 26,\n",
       " 'only': 15,\n",
       " 'possible': 16,\n",
       " 'with': 28}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     \n",
    "\n",
    "corpus = [\n",
    "    \"Secondly, Science made us reach to the moon.\",\n",
    "    \"But we never stopped there.\",\"It also gave us a glance at Mars.\",\n",
    "    \"This is one of the greatest achievements.\",\n",
    "    \"This was only possible with Science.\"]\n",
    "     \n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "     \n",
    "\n",
    "matrix = vectorizer.fit(corpus)\n",
    "matrix.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ed9aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t0.3059738056794289\n",
      "  (0, 24)\t0.3792466455261381\n",
      "  (0, 21)\t0.3059738056794289\n",
      "  (0, 19)\t0.3792466455261381\n",
      "  (0, 18)\t0.3059738056794289\n",
      "  (0, 17)\t0.3792466455261381\n",
      "  (0, 11)\t0.3792466455261381\n",
      "  (0, 9)\t0.3792466455261381\n",
      "  (1, 27)\t0.4472135954999579\n",
      "  (1, 22)\t0.4472135954999579\n",
      "  (1, 20)\t0.4472135954999579\n",
      "  (1, 12)\t0.4472135954999579\n",
      "  (1, 3)\t0.4472135954999579\n",
      "  (2, 25)\t0.3128396318588854\n",
      "  (2, 10)\t0.38775666010579296\n",
      "  (2, 8)\t0.38775666010579296\n",
      "  (2, 5)\t0.38775666010579296\n",
      "  (2, 4)\t0.38775666010579296\n",
      "  (2, 2)\t0.38775666010579296\n",
      "  (2, 1)\t0.38775666010579296\n",
      "  (3, 23)\t0.32138757599667\n",
      "  (3, 21)\t0.32138757599667\n",
      "  (3, 14)\t0.3983516165374428\n",
      "  (3, 13)\t0.3983516165374428\n",
      "  (3, 7)\t0.3983516165374428\n",
      "  (3, 6)\t0.3983516165374428\n",
      "  (3, 0)\t0.3983516165374428\n",
      "  (4, 28)\t0.43429718303084847\n",
      "  (4, 26)\t0.43429718303084847\n",
      "  (4, 23)\t0.3503882327118585\n",
      "  (4, 18)\t0.3503882327118585\n",
      "  (4, 16)\t0.43429718303084847\n",
      "  (4, 15)\t0.43429718303084847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_matrix = vectorizer.transform(corpus)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8e4ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achievements' 'also' 'at' 'but' 'gave' 'glance' 'greatest' 'is' 'it'\n",
      " 'made' 'mars' 'moon' 'never' 'of' 'one' 'only' 'possible' 'reach'\n",
      " 'science' 'secondly' 'stopped' 'the' 'there' 'this' 'to' 'us' 'was' 'we'\n",
      " 'with']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
